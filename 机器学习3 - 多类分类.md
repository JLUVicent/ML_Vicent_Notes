# 机器学习3 - 多类分类

## 问题描述

**通过逻辑回归来进行手写数字识别(0-9)**

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
#通过scipy加载matlab型数据
from scipy.io import loadmat、

data = loadmat('ex3data1.mat')#该命令用于载入mat数据（mat数据是matlab格式）
data
```

>data为字典型数据，输出结果如下：
>
>```
>{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',
> '__version__': '1.0',
> '__globals__': [],
> 'X': array([[0., 0., 0., ..., 0., 0., 0.],
>        [0., 0., 0., ..., 0., 0., 0.],
>        [0., 0., 0., ..., 0., 0., 0.],
>        ...,
>        [0., 0., 0., ..., 0., 0., 0.],
>        [0., 0., 0., ..., 0., 0., 0.],
>        [0., 0., 0., ..., 0., 0., 0.]]),
> 'y': array([[10],
>        [10],
>        [10],
>        ...,
>        [ 9],
>        [ 9],
>        [ 9]], dtype=uint8)}
>```

```python
data['X'].shape, data['y'].shape #查看数据维度
#((5000, 400), (5000, 1))
```

原始图像为20*20像素，因此可以得到400维向量特征是原始图像中每个像素的灰度强度

***

**分析问题**

将逻辑回归修改为完全向量化（没有for循环）。其优势为：简洁，可以利用线性代数优化，比循环代码快，在之前练习中已经实现

### sigmoid函数

$$
g(z)=\frac{1}{1+e^{-z}}
$$

逻辑回归模型的假设函数：
$$
h_\theta(x)=\frac{1}{1+e^{-\theta^TX}}
$$

```python
def sigmoid(z):
    return 1/(1+np.exp(-z))
```

代价函数：
$$
J(\theta)=\frac{1}{m}\sum_{i=1}^{m}[-y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)})log(1-h_\theta(x^{(i)}))]
$$

```python
def cost(theta, X, y, learningRate):
    theta = np.matrix(theta)#转换为矩阵格式
    X = np.matrix(X)
    y = np.matrix(y)
    first = np.multiply(-y, np.log(sigmoid(X * theta.T)))
    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))
    reg = (learningRate / (2 * len(X))) * np.sum(np.power(theta[:,1:theta.shape[1]], 2))#正则化项
    return np.sum(first - second) / len(X) + reg
```

梯度下降算法表示：

![循环梯度](https://cdn.jsdelivr.net/gh/JLUVicent/image-saving@master/20210731/循环梯度.62w6sngg9yg0.jpg)

for循环梯度函数：

```python
def gradient_with_loop(theta, X, y, learningRate):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)
    
    parameters = int(theta.ravel().shape[1])#theta展开为一维后的列数
    grad = np.zeros(parameters)
    
    error = sigmoid(X * theta.T) - y
    
    for i in range(parameters):
        term = np.multiply(error, X[:,i])
        
        if (i == 0):
            grad[i] = np.sum(term) / len(X)
        else:
            grad[i] = (np.sum(term) / len(X)) + ((learningRate / len(X)) * theta[:,i])
    
    return grad
```

向量化梯度函数：

```python
def gradient(theta, X, y, learningRate):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)
    
    parameters = int(theta.ravel().shape[1])
    error = sigmoid(X * theta.T) - y
    
    grad = ((X.T * error) / len(X)).T + ((learningRate / len(X)) * theta)
    
    # intercept gradient is not regularized
    grad[0, 0] = np.sum(np.multiply(error, X[:,0])) / len(X)
    
    return np.array(grad).ravel()
```

### 一对多分类函数

```python
from scipy.optimize import minimize
#通过调用scipy中的API来最小化每个分类器的代价函数。
def one_vs_all(X, y, num_labels, learning_rate):
    rows = X.shape[0]
    params = X.shape[1]
    
    # k X (n + 1) array for the parameters of each of the k classifiers X(n+1)数组作为k分类器参数
    all_theta = np.zeros((num_labels, params + 1))
    
    # insert a column of ones at the beginning for the intercept term  在截距项的开头加一列
    X = np.insert(X, 0, values=np.ones(rows), axis=1)
    
    # labels are 1-indexed instead of 0-indexed 索引从1开始而不是0
    for i in range(1, num_labels + 1):
        theta = np.zeros(params + 1)
        y_i = np.array([1 if label == i else 0 for label in y])
        y_i = np.reshape(y_i, (rows, 1))#reshape将函数变为
        
        # minimize the objective function
        fmin = minimize(fun=cost, x0=theta, args=(X, y_i, learning_rate), method='TNC', jac=gradient)
        all_theta[i-1,:] = fmin.x
    
    return all_theta
```

```python
rows = data['X'].shape[0]#取行数
params = data['X'].shape[1]#取列数
#必须确保输入矩阵维度正确
all_theta = np.zeros((10, params + 1))

X = np.insert(data['X'], 0, values=np.ones(rows), axis=1)

theta = np.zeros(params + 1)

y_0 = np.array([1 if label == 0 else 0 for label in data['y']])
print(data['y'])

y_0 = np.reshape(y_0, (rows, 1))

X.shape, y_0.shape, theta.shape, all_theta.shape

np.unique(data['y'])#查看有几类标签
```

运行一对多分类函数：

```
all_theta=one_vs_all(data['X'],data['y'],10,1)
all_theta
```

> 输入如下
>
> ```
> array([[-2.38291346e+00,  0.00000000e+00,  0.00000000e+00, ...,
>          1.30474743e-03, -8.24531507e-10,  0.00000000e+00],
>        [-3.18403111e+00,  0.00000000e+00,  0.00000000e+00, ...,
>          4.46025411e-03, -5.08531235e-04,  0.00000000e+00],
>        [-4.80012163e+00,  0.00000000e+00,  0.00000000e+00, ...,
>         -2.88116417e-05, -2.47992614e-07,  0.00000000e+00],
>        ...,
>        [-7.98722644e+00,  0.00000000e+00,  0.00000000e+00, ...,
>         -8.95295280e-05,  7.21832616e-06,  0.00000000e+00],
>        [-4.57235036e+00,  0.00000000e+00,  0.00000000e+00, ...,
>         -1.33349450e-03,  9.96261157e-05,  0.00000000e+00],
>        [-5.40509120e+00,  0.00000000e+00,  0.00000000e+00, ...,
>         -1.16616386e-04,  7.88424336e-06,  0.00000000e+00]])
> ```

### 预测每个图像的标签

使用训练完毕的分类器预测每个图像的标签，计算每个类的类概率，对于每个训练样本，将输出类标签为具有最高概率的类

```python
def predict_all(X, all_theta):
    rows = X.shape[0]
    params = X.shape[1]
    num_labels = all_theta.shape[0]
    
    # same as before, insert ones to match the shape
    X = np.insert(X, 0, values=np.ones(rows), axis=1)
    
    # convert to matrices 转换到矩阵
    X = np.matrix(X)
    all_theta = np.matrix(all_theta)
    
    # compute the class probability for each class on each training instance 
    h = sigmoid(X * all_theta.T)
    
    # create array of the index with the maximum probability 创建有最大概率的索引数组
    h_argmax = np.argmax(h, axis=1)#找到每一行的最大值对应的索引值，
    
    #由于索引数组是0所以需要为真正的预测标签加1
    h_argmax = h_argmax + 1
    
    return h_argmax#出现疑问：这一块返回的是标签为何在下面调用函数的时候返回的却是标签对应的值
```

使用预测函数进行预测，查看分类器工作过程

```python
y_pred = predict_all(data['X'], all_theta)
# print(y_pred)
# print(data['y'])
# print(zip(y_pred,data['y']))
correct = [1 if a == b else 0 for (a, b) in zip(y_pred, data['y'])]
accuracy = (sum(map(int, correct)) / float(len(correct)))
print ('accuracy = {0}%'.format(accuracy * 100))
```

> 结果：
>
> ```
> [[10]
>  [10]
>  [10]
>  ...
>  [ 9]
>  [ 9]
>  [ 7]]
> [[10]
>  [10]
>  [10]
>  ...
>  [ 9]
>  [ 9]
>  [ 9]]
> <zip object at 0x000001B100D26EC0>
> accuracy = 94.46%
> ```

