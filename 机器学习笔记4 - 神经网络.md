# 机器学习笔记4 - 神经网络

线性回归和逻辑回归都有一个缺点，当特征太多，计算负荷会很大，

### 引入神经网络

在神经网络中，参数称为权重(weight)

![4-1](https://cdn.jsdelivr.net/gh/JLUVicent/image-saving@master/20210731/4-1.3wxrmz38tai0.png)

其中$x_1,x_2,x_3$是输入单元，$a_1,a_2,a_3$是中间单元，负责处理数据传递到下一层，最后是输出单元，其用来计算$h_\theta(x)$。

在一个三层网络中，第一层成为<font color='red'>输入层（ Input Layer）</font>，最后一层称为<font color='red'>输出层（ Output Layer）</font>，中间一层成为<font color='red'>隐藏层 (Hidden Layers）</font>。我们为每一层都增加一个<font color='red'>偏置单元(bias unit)</font>，得到如下图:

![4-2](https://cdn.jsdelivr.net/gh/JLUVicent/image-saving@master/20210731/4-2.4czy52ordsy0.png)

$a^{(j)}_i$表示第$j$层的第$i$个激活单元。$\theta^{(j)}$代表从第$j$层映射到第$j+1$层时的权重矩阵，比如<font color='red'>$\theta^{(1)}$</font>表示从第一层映射到第二层的<font color='red'>权重矩阵</font>。该矩阵的大小为：以第$j+1$层的激活单元数量为<font color='red'>行数</font>，以第$j$层激活单元数加1为<font color='red'>列数</font>的矩阵。例：上图神经网络中$\theta^{(1)}$​尺寸为3*4：

对于上图所示的模型，激活单元和输出分别表达为：

![4-3](https://cdn.jsdelivr.net/gh/JLUVicent/image-saving@master/20210731/4-3.7420ovsz1qc0.png)

<font color='red'>前向传播算法：</font>从左向右的算法，（每一个a都是由上一层所有的$x$和每一个$x$所对一个的决定的）

把$x,\theta,a$​分别用矩阵表示，我们可以得到$\theta*X=a$:

![4-4](https://cdn.jsdelivr.net/gh/JLUVicent/image-saving@master/20210731/4-4.230oq1kqlyzk.png)

**我们使用向量化的方法来代替循环编码**

如下神经网络，我们来计算第二层的值：

![4-5](https://cdn.jsdelivr.net/gh/JLUVicent/image-saving@master/20210731/4-5.23v7kxwyzczk.png)

上述计算完成后添加$a^{(2)}_0=1$​,计算输出值如下：

![4-6](https://cdn.jsdelivr.net/gh/JLUVicent/image-saving@master/20210731/4-6.11h6442useio.png)

令$z^{(3)}=\theta^{(2)}a^{(2)}$,则$h_\theta(x)=a^{(3)}=g(z^{(3)})$.

如上是针对一个训练实例的计算，对整个训练集进行计算需要将训练集的特征矩阵进行转置，使得同一个实例的特征都在同一列，即：
$$
z^{(2)}=\theta^{(1)}*X^T
$$

$$
a^{(2)}=g(z^{(2)})
$$

如下图，当把神经网络的左半部分遮住，右半部分其实就是以$a_0,a_1,a_2,a_3$​,按照逻辑回归的方式输出$h_\theta(x)$​​。

![4-7](https://cdn.jsdelivr.net/gh/JLUVicent/image-saving@master/20210731/4-7.7a53nop12lk0.png)

<font color='red'>神经网络其实就是逻辑回归，只是我们把逻辑回归的输入变量变成了中间层，即:</font>

![4-8](https://cdn.jsdelivr.net/gh/JLUVicent/image-saving@master/20210731/4-8.qxvwwdfpsb4.png)

我们可以将$a_0,a_1,a_2,a_3$看成相比于$x_0,x_1,x_2,x_3$更高级的特征值，是$x$的进化，其能更好的预测新数据​

### 特征的直观理解

在神经网络中，原始特征是输入层。

如下通过单层神经元表示逻辑运算，比如逻辑与(AND)，逻辑或(OR).

用如下神经网络来表示<font color='red'>AND函数</font>

![4-9](https://cdn.jsdelivr.net/gh/JLUVicent/image-saving@master/20210731/4-9.6xjkr0t313g0.png)

其中$\theta_0=-30,\theta_1=20,\theta_2=20$,输出函数$h_\theta(x)$为$h_\theta(x)=g(-30+20x_1+20x_2)$

已知$g(x)$​图像是：

![4-10](https://cdn.jsdelivr.net/gh/JLUVicent/image-saving@master/20210731/4-10.7fpyurbgoi80.png)

得到真值表：

![4-11](https://cdn.jsdelivr.net/gh/JLUVicent/image-saving@master/20210731/4-11.265bv50uka74.png)

同理对于<font color='red'>OR函数</font>：

![4-12](https://cdn.jsdelivr.net/gh/JLUVicent/image-saving@master/20210731/4-12.414po3v7xya0.png)

整体一样，$\theta$​取值不同。

<font color='red'>逻辑非(NOT)函数：</font>

![4-13](https://cdn.jsdelivr.net/gh/JLUVicent/image-saving@master/20210731/4-13.3x4nrmnfcko0.png)

XNOR函数（输入的两个值必须一样，均为1或均为0），即

$XNOR=(x_1ANDx_2)OR((NOTx_1)AND(NOTx_2))$

首先得到$(NOTx_1)AND(NOTx_2)$的神经元：

![4-14](https://cdn.jsdelivr.net/gh/JLUVicent/image-saving@master/20210731/4-14.7c1j86m9e1c0.png)

将$AND$神经元与$(NOTx_1)AND(NOTx_2)$​的神经元以及$OR$​神经元进行组合，

![4-15](https://cdn.jsdelivr.net/gh/JLUVicent/image-saving@master/20210731/4-15.5uw2zp25awc0.png)

<font color='red'>实现了XNOR运算功能的神经网络</font>

神经网络的优势在于可以构造出很多复杂的杉树，得到更加厉害的特征值。

### 多类分类问题

![4-16](https://cdn.jsdelivr.net/gh/JLUVicent/image-saving@master/20210731/4-16.2sgegzrprme0.png)

